{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('venv': venv)",
   "display_name": "Python 3.8.6 64-bit ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "0dfcb9f2b2a0c666d30c2b07ec94473575108cbe7f3fcb7fa88a3eba4dbd17a6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Install dependencies and download spaCy's [`en_core_web_sm` model](https://spacy.io/models/en#en_core_web_sm), which is trained on a small corpus of general English text on the web. Import libraries we'll use."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: spacy in ./venv/lib/python3.8/site-packages (2.3.2)\n",
      "Collecting unidecode\n",
      "  Using cached Unidecode-1.1.1-py2.py3-none-any.whl (238 kB)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.8/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./venv/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.8/site-packages (from spacy) (4.51.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in ./venv/lib/python3.8/site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.8/site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from spacy) (49.2.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.8/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.1.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/nbudak/src/geniza/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in ./venv/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in ./venv/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.51.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/nbudak/src/geniza/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy unidecode\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import csv        # loading/saving data\n",
    "import spacy      # nlp library\n",
    "import difflib    # comparing lists of terms\n",
    "import unidecode  # normalizing terms for comparison\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import islice"
   ]
  },
  {
   "source": [
    "Load the spaCy model and define the types of entities we are interested in recognizing. Create `Counter` objects to track the number of times we see entities."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ent_types = [\"GPE\", \"PERSON\"]       # \"GPE\" = geopolitical entity (place)\n",
    "ent_counts = defaultdict(Counter)   # stats container"
   ]
  },
  {
   "source": [
    "Load all items from the PGP Metadata spreadsheet, storing their descriptions and PGPIDs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loaded 29946 items\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "with open(\"pgp_items.csv\") as file:\n",
    "  reader = csv.DictReader(file)\n",
    "  for row in reader:\n",
    "    items.append((\n",
    "        row[\"Description\"],\n",
    "        { \"PGPID\": row[\"\\ufeffPGPID\"], }\n",
    "    ))\n",
    "print(f\"loaded {len(items)} items\")"
   ]
  },
  {
   "source": [
    "Process all item descriptions using our spaCy pipeline, counting occurrences of named entities as we encounter them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing completed\n"
     ]
    }
   ],
   "source": [
    "for doc, context in nlp.pipe(items, as_tuples=True, disable=[\"tagger\", \"parser\"]):  # only run NER part of pipe\n",
    "  for ent in doc.ents:\n",
    "    ent_counts[ent.label_][ent.text] += 0.5     # creates one Counter for each entity type\n",
    "print(\"processing completed\")"
   ]
  },
  {
   "source": [
    "Load the list of known places, and compare it to the places we identified to see which ones aren't in the list. Of those that weren't listed, check how common they are in item descriptions. Write the results to a file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "identified 1036 potential places not in list\nwrote missing_places.csv\n"
     ]
    }
   ],
   "source": [
    "known_places = []\n",
    "with open(\"pgp_places.csv\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in islice(reader, 1, None):   # skip header row\n",
    "        for cell in row:\n",
    "            if cell:                      # skip blank cells\n",
    "                known_places.append(cell.lower().strip())\n",
    "\n",
    "# normalize both lists by removing whitespace and lowercasing\n",
    "places = [place.lower().strip() for place in ent_counts[\"GPE\"].keys()]\n",
    "\n",
    "# some markers clearly indicate a person has been misidentified as a place\n",
    "person_markers = [\"b.\", \"bat\", \"abÅ« \", \"abu \", \"abu-\"]\n",
    "places = [place for place in places if not any([marker in place for marker in person_markers])]\n",
    "\n",
    "# if there are numbers present it's likely a date, not a place\n",
    "places = [place for place in places if not any([char.isdigit() for char in place])]\n",
    "\n",
    "# normalize unicode (e.g. diacritics)\n",
    "places = [unidecode.unidecode(place) for place in places]\n",
    "\n",
    "# ensure we only have unique values after normalization\n",
    "places = list(set(places))\n",
    "\n",
    "# compare the two lists and keep those we didn't find in the known places list\n",
    "missing = []\n",
    "for line in difflib.ndiff(sorted(places), sorted(known_places)):\n",
    "    if line.startswith(\"-\"):\n",
    "        missing.append(line[2:])\n",
    "print(f\"identified {len(missing)} potential places not in list\")\n",
    "\n",
    "# get the most frequently occurring missing places\n",
    "norm_counts = Counter({ unidecode.unidecode(place.lower().strip()): count for (place, count) in ent_counts[\"GPE\"].items() })\n",
    "missing_counts = Counter({ place: count for (place, count) in norm_counts.items() if place in missing })\n",
    "\n",
    "# write the results to a file\n",
    "with open(\"missing_places.csv\", mode=\"w\") as file:\n",
    "    fieldnames = [\"place\", \"count\"]\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for place, count in missing_counts.most_common():       # order by count\n",
    "        writer.writerow({ \"place\": place, \"count\": count })\n",
    "print(\"wrote missing_places.csv\")\n",
    "\n"
   ]
  }
 ]
}