{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "e705b2f8f35aec4a24e2e1bafed902f26d1f6a3f190e14c8b83e9d850bffefd1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Install dependencies and download spaCy's [`en_core_web_sm` model](https://spacy.io/models/en#en_core_web_sm), which is trained on a small corpus of general English text on the web. Import libraries we'll use."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: spacy in ./venv/lib/python3.8/site-packages (2.3.2)\n",
      "Requirement already satisfied: unidecode in ./venv/lib/python3.8/site-packages (1.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.8/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.8/site-packages (from spacy) (4.51.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./venv/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in ./venv/lib/python3.8/site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.8/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from spacy) (49.2.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.8/site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in ./venv/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in ./venv/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.51.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy unidecode\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import csv        # loading/saving data\n",
    "import spacy      # nlp library\n",
    "import difflib    # comparing lists of terms\n",
    "import unidecode  # normalizing terms for comparison\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import islice"
   ]
  },
  {
   "source": [
    "Load the spaCy model and define the types of entities we are interested in recognizing. Create `Counter` objects to track the number of times we see entities."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ent_types = [\"GPE\", \"PERSON\"]       # \"GPE\" = geopolitical entity (place)\n",
    "ent_counts = defaultdict(Counter)   # stats container\n",
    "ents = defaultdict(lambda: defaultdict(list))            # lists of identified entities"
   ]
  },
  {
   "source": [
    "Load all items from the PGP Metadata spreadsheet, storing their descriptions and PGPIDs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loaded 29242 items\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "with open(\"pgp_items.csv\") as file:\n",
    "  reader = csv.DictReader(file)\n",
    "  for row in reader:\n",
    "    items.append((\n",
    "        row[\"Description\"],\n",
    "        { \"PGPID\": row[\"\\ufeffPGPID\"], }\n",
    "    ))\n",
    "print(f\"loaded {len(items)} items\")"
   ]
  },
  {
   "source": [
    "Create a normalization function for comparing entities that strips out diacritics, lowercases, and removes whitespace. Also add a custom property to identified entities to store their normalized forms and PGPID where they occur."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "[E090] Extension 'norm_text' already exists on Span. To overwrite the existing extension, set `force=True` on `Span.set_extension`.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-795067e6922b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pgpid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"norm_text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.set_extension\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E090] Extension 'norm_text' already exists on Span. To overwrite the existing extension, set `force=True` on `Span.set_extension`."
     ]
    }
   ],
   "source": [
    "def norm_text(text):\n",
    "    return unidecode.unidecode(text.lower().strip())\n",
    "spacy.tokens.Span.set_extension(\"pgpid\", default=\"\")\n",
    "spacy.tokens.Span.set_extension(\"norm_text\", default=\"\")"
   ]
  },
  {
   "source": [
    "Process all item descriptions using our spaCy pipeline, counting and storing occurrences of named entities as we encounter them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing completed\n"
     ]
    }
   ],
   "source": [
    "for doc, context in nlp.pipe(items, as_tuples=True, disable=[\"tagger\", \"parser\"]):  # only run NER part of pipe\n",
    "  for ent in doc.ents:\n",
    "    ent._.norm_text = norm_text(ent.text)          # normalize entity text\n",
    "    ent._.pgpid = context[\"PGPID\"]                 # store pgpid on entity\n",
    "    ents[ent.label_][ent._.norm_text].append(ent)  # list of occurrences in docs for each named entity, sorted by type \n",
    "    ent_counts[ent.label_][ent._.norm_text] += 1   # count of occurrences of each named entity, sorted by type\n",
    "print(\"processing completed\")"
   ]
  },
  {
   "source": [
    "Load the list of known places."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loaded 224 known places\n"
     ]
    }
   ],
   "source": [
    "known_places = []\n",
    "with open(\"pgp_places.csv\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in islice(reader, 1, None):   # skip header row\n",
    "        for cell in row:\n",
    "            if cell:                      # skip blank cells\n",
    "                known_places.append(norm_text(cell))    # normalize\n",
    "known_places = list(set(known_places))    # dedup\n",
    "print(f\"loaded {len(known_places)} known places\")"
   ]
  },
  {
   "source": [
    "Further process the list of places to prevent false positives."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "identified 1166 places after normalization\n"
     ]
    }
   ],
   "source": [
    "# GPE = geopolitical entity (place)\n",
    "places = ents[\"GPE\"]\n",
    "\n",
    "# some markers clearly indicate a person has been misidentified as a place\n",
    "person_markers = [\"b. \", \"bt. \", \"ibn \", \"bint \", \"ben \", \"bat \", \"abÅ« \", \"abu \", \"abu-\", \"bu \", \"umm \", \"sitt \"]\n",
    "\n",
    "output = defaultdict(list)\n",
    "for name, occurrences in places.items():\n",
    "    # check if any clear person markers or numbers are present;\n",
    "    # if there are numbers present it's likely a date, not a place\n",
    "    if not any([m in name for m in person_markers]) and \\\n",
    "       not any([c.isdigit() for c in name]):    \n",
    "       output[name] = occurrences\n",
    "places = dict(output)\n",
    "place_counts = dict(ent_counts[\"GPE\"])\n",
    "\n",
    "print(f\"identified {len(places)} places after normalization\")"
   ]
  },
  {
   "source": [
    "Compare the two lists, checking how many times places were automatically identified that weren't in the known list of places."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "identified 1034 potential places not in list\n"
     ]
    }
   ],
   "source": [
    "# compare the two lists and keep those we didn't find in the known places list\n",
    "missing = []\n",
    "for line in difflib.ndiff(sorted(places), sorted(known_places)):\n",
    "    if line.startswith(\"-\"):\n",
    "        missing.append(line[2:])\n",
    "print(f\"identified {len(missing)} potential places not in list\")\n",
    "\n",
    "# get the most frequently occurring missing places\n",
    "missing_counts = { place: count for (place, count) in ent_counts[\"GPE\"].items() if place in missing }"
   ]
  },
  {
   "source": [
    "Write results to a file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "wrote missing_places.csv\n"
     ]
    }
   ],
   "source": [
    "# write the results to a file\n",
    "with open(\"missing_places.csv\", mode=\"w\") as file:\n",
    "    fieldnames = [\"place\", \"count\", \"pgpid\"]\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for place, count in reversed(sorted(missing_counts.items(), key=lambda e: e[1])):       # order by count\n",
    "        pgpids = list(set([o._.pgpid for o in ents[\"GPE\"][place] if o._.pgpid.strip() != \"\"]))\n",
    "        writer.writerow({ \"place\": place, \"count\": count, \"pgpid\": \", \".join(pgpids) })\n",
    "print(\"wrote missing_places.csv\")"
   ]
  },
  {
   "source": [
    "Load the lists of known authors/editors into a single list of known people."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loaded 8893 known people\n"
     ]
    }
   ],
   "source": [
    "known_people = []\n",
    "for sheet in [\"pgp_authors.csv\", \"pgp_editors.csv\"]:\n",
    "    with open(sheet) as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            known_people.append(norm_text(row[\"Name PGP\"])) # normalize\n",
    "known_people = list(set(known_people)) # dedup\n",
    "print(f\"loaded {len(known_people)} known people\")"
   ]
  },
  {
   "source": [
    "Further process the list of identified people to prevent false positives."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "identified 11535 people after normalization\n"
     ]
    }
   ],
   "source": [
    "people = ents[\"PERSON\"]\n",
    "\n",
    "# add additional context to people's names since the recognizer doesn't do a great job with arabic names\n",
    "output = defaultdict(list)\n",
    "for name, occurrences in people.items():\n",
    "    # if there are numbers present it's likely a date, not a person\n",
    "    if not any([c.isdigit() for c in name]):\n",
    "        # for names starting with common markers, add the two tokens to the left as context\n",
    "        # note that this will often make it not match the known names, but there's little overlap anyway\n",
    "        if any([name.startswith(m) for m in person_markers]):\n",
    "            for o in occurrences:\n",
    "                name_with_context = str(o.doc[max(o.start - 2, 0):o.end])\n",
    "                output[name_with_context].append(o)                 # track occurrences for contextualized versions\n",
    "                ent_counts[\"PERSON\"][name_with_context] += 1        # track counts for contextualized versions\n",
    "        # for other names just keep the saved occurrences\n",
    "        else:\n",
    "            output[name] = occurrences\n",
    "people = dict(output)\n",
    "\n",
    "print(f\"identified {len(people)} people after normalization\")"
   ]
  },
  {
   "source": [
    "Compare the two lists, saving the identified people who weren't a known author or editor. Check how many times the missing people were identified in descriptions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "identified 10435 potential people not in list\n"
     ]
    }
   ],
   "source": [
    "# compare the two lists and keep those we didn't find in the known people list\n",
    "missing = []\n",
    "for line in difflib.ndiff(sorted(people), sorted(known_people)):\n",
    "    if line.startswith(\"-\"):\n",
    "        missing.append(line[2:])\n",
    "print(f\"identified {len(missing)} potential people not in list\")\n",
    "\n",
    "# get the most frequently occurring missing people\n",
    "missing_counts = Counter({ person: count for (person, count) in ent_counts[\"PERSON\"].items() if person in missing })"
   ]
  },
  {
   "source": [
    "Write the results to a file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "wrote missing_people.csv\n"
     ]
    }
   ],
   "source": [
    "# write the results to a file\n",
    "with open(\"missing_people.csv\", mode=\"w\") as file:\n",
    "    fieldnames = [\"person\", \"count\", \"pgpid\"]\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for person, count in missing_counts.most_common():       # order by count\n",
    "        pgpids = list(set([o._.pgpid for o in ents[\"PERSON\"][person] if o._.pgpid.strip() != \"\"]))\n",
    "        writer.writerow({ \"person\": person, \"count\": count, \"pgpid\": \", \".join(pgpids) })\n",
    "print(\"wrote missing_people.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}